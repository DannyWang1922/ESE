model_name_or_path: google-bert/bert-base-uncased
train_split_name: train
valid_name_or_path: mteb/stsbenchmark-sts
valid_split_name: test
save_dir: ./train_result/bert_moe_base_all
seed: 42
workers: 16

# configure Aoe
cosine_w: 0.0
ibn_w: 1.0
angle_w: 0.02
cosine_tau: 20.0
ibn_tau: 20.0
angle_tau: 20.0

# configure lora
apply_lora: 0
lora_r: 32
lora_alpha: 32
lora_dropout: 0.1

# configure training
learning_rate: 5e-5
warmup_steps: 100
logging_steps: 500
pooling_strategy: cls
epochs: 10
save_steps: 500
batch_size: 16
maxlen: 512
dataset_seed: 42

# configure LLM
is_llm: 0
apply_billm: 0

# configure ESE
apply_ese: 0
ese_compression_size: 128
ese_kl_temperature: 1.0

# configure MoE
use_bert_moe: 1
moe_layers: all
max_train_samples: 50000